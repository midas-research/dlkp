{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tranKP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXDqBqrdaoNw",
        "outputId": "4e45f4d4-324f-44a0-f289-62479ccd56ef"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 17.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 21.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=5eee5bbfe2f9124d4f5d0c0332e4124d253f5989149682918253b6700d942717\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 5.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/78/5873ac1e27bf25a2cbf3447d6704edd3136b1b3ff0eb3bfab38a45d2a1ff/datasets-1.2.0-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 6.1MB/s \n",
            "\u001b[?25hCollecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, pyarrow, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.2.0 pyarrow-2.0.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3IcbMLKYCz7"
      },
      "source": [
        "# utils\n",
        "import os, sys\n",
        "import argparse\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional\n",
        "@dataclass\n",
        "class BasicKPArgs:\n",
        "    model_type : Optional[str] = field(\n",
        "        default=\"enc_dec\",\n",
        "        metadata= {\"help\": \"encoder decoder type or other generative model like Bart\"}\n",
        "    )\n",
        "\n",
        "    model_name_path : Optional[str] = field(\n",
        "        default= None,\n",
        "        metadata= {\"help\": \"path or name to load pretrained model or from checkpoints\"}\n",
        "    )\n",
        "    decoder_model_name_path : Optional[str] = field(\n",
        "        default= None,\n",
        "        metadata= {\"help\": \"path or name of decoder part of the model in enc_dec architect\"}\n",
        "    )\n",
        "    tokenizer_path  : Optional[str] = field(\n",
        "        default= None,\n",
        "        metadata= {\"help\": \"path or name of custom tokenizer saved if provided this tokenizer will be loaded else auto tokenizer\"}\n",
        "    )\n",
        "    data_dir : Optional[str] = field(\n",
        "        default= \"\",\n",
        "        metadata= {\"help\": \"path to dir containg data\"}\n",
        "    )\n",
        "    kp_task_type : Optional[str] = field(\n",
        "        default= \"one2one\",\n",
        "        metadata= {\"help\": \"wether to use one2one or one2many\"}\n",
        "    )\n",
        "    max_src_len : Optional[int] = field(\n",
        "        \n",
        "        default= 512,\n",
        "        metadata= {\"help\": \"length of source seq\" }\n",
        "    )\n",
        "    max_tar_len : Optional[int] = field(\n",
        "        \n",
        "        default= 64,\n",
        "        metadata= {\"help\": \"length of target seq\" }\n",
        "    )\n",
        "    # this is parsed from training args\n",
        "    # out_dir: Optional[str] = field(\n",
        "    #     default= \"\",\n",
        "    #     metadata= {\"help\": \"path of data dir to save trained weights and out put\"}\n",
        "    # )\n",
        "    from_pretrained : Optional[bool] = field(\n",
        "        default= True,\n",
        "        metadata= {\"help\": \"wether to load model weight from a pretrained checkpoint or from scratch\"}\n",
        "    )\n",
        "    predict_only : Optional[bool] = field(\n",
        "        default= False,\n",
        "        metadata= {\"help\": \"wether to predict only or train, validate and predict\"}\n",
        "    )\n",
        "    dataset_class : Optional[str] = field(\n",
        "        default= \"single\",\n",
        "        metadata= {\"help\": \"single | multiple , type of dataset reader to use, split train data into mltiple train file or from single\" }\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgUthX_SaXRY"
      },
      "source": [
        "#datset\n",
        "import os, sys\n",
        "import torch\n",
        "import json\n",
        "from torch.utils.data.dataset import Dataset\n",
        "class KPone2manyDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path, max_src_len, max_tar_len, kp_sep_token = \"<kp_sep>\"):\n",
        "        '''\n",
        "        file should contain json in each line with\n",
        "            \"text\": string and \" key phrase\": list[str] containing all kp\n",
        "        '''\n",
        "        assert os.path.exists(file_path)\n",
        "        self.abst= []\n",
        "        self.kps= []\n",
        "        self.src_attn_mask = []\n",
        "        self.tokenizer = tokenizer\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                d=json.loads(line)\n",
        "                self.abst.append(d['text'])\n",
        "                curr_kp= \"\"\n",
        "                for (i,kp) in enumerate(d['kp']):\n",
        "                    if i !=0:\n",
        "                        curr_kp += \" \" + kp_sep_token +\" \"\n",
        "                    curr_kp += kp.strip()\n",
        "                \n",
        "                self.kps.append(curr_kp)\n",
        "        \n",
        "        assert len(self.kps) == len(self.abst)\n",
        "        self.ex_len= len(self.abst)\n",
        "        self.kps= self.tokenizer.batch_encode_plus(self.kps, truncation=True, max_length= max_tar_len, pad_to_max_length= True)\n",
        "        self.abst= self.tokenizer.batch_encode_plus(self.abst, truncation=True, max_length= max_src_len, pad_to_max_length= True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.ex_len\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return {\n",
        "            'src_ids': torch.tensor(self.abst['input_ids'][i]),\n",
        "            'tar_ids': torch.tensor(self.kps['input_ids'][i]),\n",
        "            'src_attn': torch.tensor(self.abst['attention_mask'][i]),\n",
        "            'tar_attn': torch.tensor(self.kps['attention_mask'][i])\n",
        "            }\n",
        "\n",
        "# class kpone2manyMultiDataset(Dataset):\n",
        "#     def __init__(self, tokenizer, data_dir, file_prefix, n=10000, max_src_len, max_tar_len, kp_sep_token = \"<kp_sep>\"):\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.data_dir = data_dir\n",
        "#         self.file_prefix = file_prefix\n",
        "#         self.total_examples = n\n",
        "#         self.max_src_len = max_src_len\n",
        "#         self.max_tar_len = max_tar_len\n",
        "#         self.kp_sep_token = kp_sep_token\n",
        "\n",
        "#         assert os.path.exists(self.data_dir)\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "#         pass\n",
        "#     def read_files(self):\n",
        "#         pass\n",
        "\n",
        "#     def __len__(self):\n",
        "#         pass\n",
        "    \n",
        "#     def __getitem__(self,i):\n",
        "#         pass\n",
        "\n",
        "\n",
        "\n",
        "# super dataset class\n",
        "def load_kp_data_and_dataset_class( tokenizer, file_path, max_src_len, max_tar_len, kp_sep_token = \"<kp_sep>\"):\n",
        "    from datasets import load_dataset\n",
        "    def tok_and_process(d):\n",
        "        curr_kp= \"\"\n",
        "        for (i,kp) in enumerate(d['kp']):\n",
        "            if i !=0:\n",
        "                curr_kp += \" \" + kp_sep_token +\" \"\n",
        "            curr_kp += kp.strip()\n",
        "        src_encode= tokenizer(d['text'],  truncation=True, max_length= max_tar_len, pad_to_max_length= True)\n",
        "        tar_encode= tokenizer(curr_kp, truncation=True, max_length= max_tar_len, pad_to_max_length= True)\n",
        "        d['input_ids'] = src_encode['input_ids']\n",
        "        d['decoder_input_ids']= tar_encode['input_ids']\n",
        "        d['attention_mask']= src_encode['attention_mask']\n",
        "        # d['tar_attn'] = tar_encode['attention_mask']\n",
        "\n",
        "        return d\n",
        "    \n",
        "\n",
        "    dataset = load_dataset('json', data_files= file_path, split='train')\n",
        "    dataset= dataset.map(tok_and_process)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'decoder_input_ids', 'attention_mask'])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL3jsUs2aXUI"
      },
      "source": [
        "#collate\n",
        "import os, sys\n",
        "import torch\n",
        "class TPBDataCollator():\n",
        "    def __init__(self, tokenizer, need_to_shift= False, start_tok_id= None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.shift_right= need_to_shift\n",
        "        self.dec_start_tok_id= self.tokenizer.pad_token_id if start_tok_id is None else start_tok_id #generally same as pad token id\n",
        "\n",
        "\n",
        "    def __call__(self, ex):\n",
        "        # print(ex)\n",
        "        src_ids= torch.stack([e['input_ids'] for e in ex])\n",
        "        tar_ids= torch.stack([e['decoder_input_ids'] for e in ex])\n",
        "        src_attn_mask= torch.stack([e['attention_mask'] for e in ex])\n",
        "        # src_ids= [e['src_ids'] for e in ex]\n",
        "        # tar_ids= [e['tar_ids'] for e in ex]\n",
        "        # src_attn_mask= [e['src_attn'] for e in ex]\n",
        "        # tar_attn_mask = torch.stack([e['tar_attn'] for e in ex])\n",
        "        # create labels for loss calcualtiona\n",
        "        labels= tar_ids.clone()\n",
        "        labels[labels[:]== self.tokenizer.pad_token_id] = -100 #ignore loss at pad token ids\n",
        "\n",
        "        # get decoder input ids\n",
        "\n",
        "        if self.shift_right: # either shift right for bart/pegasus/t5 or pass decodeer ids as none for bart/ pegasus then it will create decoder ids by shifting labels to right\n",
        "            decoder_ids=  self.right_shift(tar_ids)\n",
        "            \n",
        "        else:\n",
        "            decoder_ids= tar_ids\n",
        "\n",
        "\n",
        "        batch = {\n",
        "            \"input_ids\": src_ids,\n",
        "            \"attention_mask\" : src_attn_mask,\n",
        "            \"decoder_input_ids\": decoder_ids,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "    def right_shift(self, input_ids):\n",
        "        pad_token_id= self.dec_start_tok_id  # same as pad token id\n",
        "        prev_output_tokens = input_ids.clone()\n",
        "        assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
        "        # replace possible -100 values in labels by `pad_token_id`\n",
        "        prev_output_tokens.masked_fill_(prev_output_tokens == -100, pad_token_id)\n",
        "\n",
        "        index_of_eos = (prev_output_tokens.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
        "        decoder_start_tokens = prev_output_tokens.gather(1, index_of_eos).squeeze()\n",
        "        prev_output_tokens[:, 1:] = prev_output_tokens[:, :-1].clone()\n",
        "        prev_output_tokens[:, 0] = decoder_start_tokens\n",
        "        return prev_output_tokens\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8r6v-NJaXWs"
      },
      "source": [
        "#main\n",
        "import os, sys\n",
        "# from utils import arg_parse\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    EncoderDecoderModel,\n",
        "    BartTokenizerFast,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    HfArgumentParser\n",
        ")\n",
        "\n",
        "# from dataset_fn import *\n",
        "# from collate_fn import *\n",
        "\n",
        "\n",
        "COLLATE_DICT= {\n",
        "    't5': TPBDataCollator\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "DATASET_DICT= {\n",
        "    # 'one2many_single': KPone2manyDataset\n",
        "    'one2many_single': load_kp_data_and_dataset_class\n",
        "\n",
        "}\n",
        "\n",
        "CONFIG_MAP = {\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "TOKENIZER_MAP = {\n",
        "    \n",
        "\n",
        "}\n",
        "\n",
        "MODEL_MAP = {\n",
        "\n",
        "}\n",
        "# TODO\n",
        "# modify tokenizer in main function if there is requirement of special token addition and stuff\n",
        "# chek if there is crosss ateention enabled in decoder part of the model and its working\n",
        "# see if special token needed and shifting or other requirement->>>> one at a time\n",
        "#   1. bart model\n",
        "#   2. t5\n",
        "#   3. pegasus \n",
        "# add <kp_sep> token in every tokenizer and keeep rest same, qg has better logic\n",
        "#token shifting in bart t5 pegasus\n",
        "    # t5 tokenizer genrate token as required( there is need to shift right), but bart and pegasus add cls/bos and sep/eos in start and end and it also shifts automatically\n",
        "    # for bart and pegasus simply copying target seq as labels and target seq as decodeer ip could be tried as these model automatically shift to right\n",
        "    #final: bart shifts label (i.e target seq ) to right if passed decoder ip ids is none so only labels and input ids can be passed can be passed. if you want you cann remove [cls]/[sep] token as required\n",
        "    #pegasus; same as bart\n",
        "#encode decoder: look for shifting\n",
        "    # cls can be use as bos and sep as eos: this is mentioned in HF blogs\n",
        "# how to levare seq2seq trainer or trainer directly\n",
        "    # trainer and seq2seq trainer seems to be the same thing, we can try them alternative and can see which is best\n",
        "    # \n",
        "\n",
        "# add compute metrics\n",
        "\n",
        "# add do predict and generate function option\n",
        "\n",
        "def main_fn(args= None, training_args = None):\n",
        "    #ars parsing\n",
        "    # parser= HfArgumentParser((BasicKPArgs, TrainingArguments))\n",
        "    # args , training_args = parser.parse_args_into_dataclasses()\n",
        "    \n",
        "    #load tokenizer\n",
        "    if args.tokenizer_path is not None:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)\n",
        "    else:\n",
        "        tokenizer= AutoTokenizer.from_pretrained(args.model_name_path)\n",
        "    tokenizer.add_tokens(['<kp_sep>'])\n",
        "        # tokenizer.sep_token = \"<sep>\"\n",
        "        #save tokenizer\n",
        "    tok_path= training_args.output_dir+\"/kp_{}_tokenizer\".format(args.model_name_path )\n",
        "    if not os.path.exists(tok_path):\n",
        "        os.mkdir(tok_path)\n",
        "    tokenizer.save_pretrained(tok_path)\n",
        "\n",
        "    \n",
        "    #load model\n",
        "    if args.model_type == \"enc_dec\":\n",
        "        model =None\n",
        "    else:\n",
        "        if args.from_pretrained:\n",
        "            model  = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                args.model_name_path\n",
        "            )\n",
        "        else:\n",
        "            config= AutoConfig.from_pretrained(args.model_name_path) #get the config file to load weight from scratch\n",
        "            model= AutoModelForSeq2SeqLM.from_config(config) #load model with random weight from config\n",
        "\n",
        "    #resize model embedding\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    #freeze model embedding\n",
        "\n",
        "    #datset class\n",
        "    \n",
        "    train_data_set= DATASET_DICT[args.kp_task_type+\"_\"+args.dataset_class](tokenizer= tokenizer, file_path= args.data_dir + \"/train.txt\", max_src_len=  args.max_src_len, max_tar_len = args.max_tar_len)\n",
        "\n",
        "    eval_data_set= DATASET_DICT[args.kp_task_type+\"_\"+args.dataset_class](tokenizer= tokenizer, file_path= args.data_dir+\"/val.txt\", max_src_len=  args.max_src_len, max_tar_len = args.max_tar_len)\n",
        "    \n",
        "    # print(train_data_set)\n",
        "\n",
        "    #data collator\n",
        "    data_collator= COLLATE_DICT[args.model_type](tokenizer= tokenizer, need_to_shift= True)\n",
        "\n",
        "    trainer= Trainer(model= model,\n",
        "                 args= training_args,\n",
        "                 data_collator= data_collator,\n",
        "                 train_dataset = train_data_set,\n",
        "                 eval_dataset= eval_data_set,\n",
        "                #  compute_metrics= None, # metrics to compute scores,\n",
        "\n",
        "\n",
        "                 )\n",
        "    \n",
        "    if args.predict_only:\n",
        "        test_data_set = DATASET_DICT[args.kp_task_type+\"_\"+args.dataset_class](tokenizer= tokenizer, file_path= args.data_dir+\"/test.txt\", max_src_len=  args.max_src_len, max_tar_len = args.max_tar_len)\n",
        "        \n",
        "    \n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SognzQU6aXY7"
      },
      "source": [
        "def runner():\n",
        "    args= BasicKPArgs(\n",
        "        model_type = 't5',\n",
        "        model_name_path = 't5-base', #todo\n",
        "        data_dir= \"/content\", #todo\n",
        "        kp_task_type= \"one2many\",\n",
        "        dataset_class= 'single'\n",
        "    )\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir= \"/content/tk_out\", #todo\n",
        "        overwrite_output_dir = True,\n",
        "        num_train_epochs = 2,\n",
        "        per_device_train_batch_size = 8,\n",
        "        do_eval= True,\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        save_steps = 1\n",
        "        \n",
        "        \n",
        "\n",
        "    )\n",
        "    main_fn(args, training_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "EMXA4Yi6aXbV",
        "outputId": "f091bbbd-d403-48a9-8089-ce746dd66cb3"
      },
      "source": [
        "runner()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
            "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Using custom data configuration default\n",
            "Reusing dataset json (/root/.cache/huggingface/datasets/json/default-0a0c845d87888c0a/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514)\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-0a0c845d87888c0a/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514/cache-463e16185536cbff.arrow\n",
            "Using custom data configuration default\n",
            "Reusing dataset json (/root/.cache/huggingface/datasets/json/default-ea1a5f71c165584a/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-ea1a5f71c165584a/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514/cache-cea35ead7b156669.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='2' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/6 : < :, Epoch 0.33/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2a03703c532d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-d35da4e323f1>\u001b[0m in \u001b[0;36mrunner\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     )\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-bd31ca55a059>\u001b[0m in \u001b[0;36mmain_fn\u001b[0;34m(args, training_args)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_epoch_stop\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0mreissue_pt_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaught_warnings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_world_process_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"optimizer.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scheduler.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:274] . unexpected pos 1606179904 vs 1606179792"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CaaHimjIfOR"
      },
      "source": [
        "# 3 eval_kp.py\n",
        "import os, sys\n",
        "# from utils import arg_parse\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    EncoderDecoderModel,\n",
        "    BartTokenizerFast,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    HfArgumentParser\n",
        ")\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EvalArgs:\n",
        "    model_type : Optional[str] = field(\n",
        "        default=\"enc_dec\",\n",
        "        metadata= {\"help\": \"encoder decoder type or other generative model like Bart\"}\n",
        "    )\n",
        "\n",
        "    model_name_path : Optional[str] = field(\n",
        "        default= None,\n",
        "        metadata= {\"help\": \"path or name to load pretrained model or from checkpoints\"}\n",
        "    )\n",
        "    tokenizer_path  : Optional[str] = field(\n",
        "        default= None,\n",
        "        metadata= {\"help\": \"path or name of custom tokenizer saved if provided this tokenizer will be loaded else auto tokenizer\"}\n",
        "    )\n",
        "    data_dir : Optional[str] = field(\n",
        "        default= \"\",\n",
        "        metadata= {\"help\": \"path to dir containg data\"}\n",
        "    )\n",
        "    kp_task_type : Optional[str] = field(\n",
        "        default= \"one2one\",\n",
        "        metadata= {\"help\": \"wether to use one2one or one2many\"}\n",
        "    )\n",
        "    dataset_class : Optional[str] = field(\n",
        "        default= \"single\",\n",
        "        metadata= {\"help\": \"single | multiple , type of dataset reader to use, split train data into mltiple train file or from single\" }\n",
        "    )\n",
        "    beam_size : Optional[int] = field(\n",
        "        \n",
        "        default= 4,\n",
        "        metadata= {\"help\": \"beam_size\" }\n",
        "    )\n",
        "    max_pre_len : Optional[int] = field(\n",
        "        \n",
        "        default= 64,\n",
        "        metadata= {\"help\": \"length of target seq\" }\n",
        "    )\n",
        "    max_src_len : Optional[int] = field(\n",
        "        \n",
        "        default= 512,\n",
        "        metadata= {\"help\": \"length of source seq\" }\n",
        "    )\n",
        "\n",
        "COLLATE_DICT= {\n",
        "    't5': TPBDataCollator\n",
        "\n",
        "\n",
        "}\n",
        "def main_eval(args= None):\n",
        "    # p = HfArgumentParser((EvalArgs,))\n",
        "    # args= p.parse_args_into_dataclasses()[0]\n",
        "   \n",
        "\n",
        "    device= 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "    print(\"device \", device)\n",
        "    if args.tokenizer_path is not None:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path )\n",
        "    else:\n",
        "        tokenizer= AutoTokenizer.from_pretrained(args.model_name_path)\n",
        "    tokenizer.add_tokens(['<kp_sep>'])\n",
        "\n",
        "    if args.model_type == \"enc_dec\":\n",
        "        model =None\n",
        "    else:\n",
        "        model  = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            args.model_name_path\n",
        "        )\n",
        "    data_collator= COLLATE_DICT[args.model_type](tokenizer= tokenizer, need_to_shift= True)\n",
        "    test_data_set = DATASET_DICT[args.kp_task_type+\"_\"+args.dataset_class](tokenizer= tokenizer, file_path= args.data_dir+\"/test.txt\", max_src_len=  args.max_src_len, max_tar_len = args.max_pre_len)\n",
        "\n",
        "    data_loader= torch.utils.data.DataLoader(test_data_set, batch_size= 16, collate_fn= data_collator)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    out_writer= open(args.data_dir+\"prediction.txt\", 'w')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ex in data_loader:\n",
        "            generated= model.generate(\n",
        "                input_ids= ex['input_ids'].to(device),\n",
        "                attention_mask= ex['attention_mask'].to(device),\n",
        "                num_beams= args.beam_size,\n",
        "                max_length= args.max_pre_len\n",
        "\n",
        "\n",
        "            )\n",
        "\n",
        "            pre= [tokenizer.decode(op, skip_special_token= True) for op in generated]\n",
        "            for p in pre:\n",
        "                out_writer.write(p+\"\\n\")\n",
        "            \n",
        "    \n",
        "    print(\"files written in dir {} as prediction.txt \".format(args.data_dir))\n",
        "\n",
        "    out_writer.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP6VdOBZIfRi"
      },
      "source": [
        "args= EvalArgs(\n",
        "        model_type= 't5',\n",
        "        model_name_path= \"/content/tk_out/checkpoint-6\",\n",
        "        tokenizer_path=  \"/content/tk_out/kp_t5-base_tokenizer/\",\n",
        "        data_dir= \"/content/\", #todo\n",
        "        kp_task_type= \"one2many\",\n",
        "        dataset_class= 'single',\n",
        "        beam_size= 4,\n",
        "        max_pre_len = 64\n",
        "\n",
        "\n",
        "    )\n",
        "main_eval(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU7vhi4nbnmx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joA6Zo0SIfUf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVByVZnaaXdl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}