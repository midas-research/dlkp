{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dlkp.models import KeyphraseTagger\n",
    "from dlkp.extraction import (\n",
    "    KEDataArguments,\n",
    "    KEModelArguments,\n",
    "    KETrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Initialize the data arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"midas/inspec\"\n",
    "data_args = KEDataArguments(\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_config_name=\"extraction\",\n",
    "    pad_to_max_length=True,\n",
    "    overwrite_cache=True,\n",
    "    preprocessing_num_workers=8,\n",
    "    return_entity_level_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Initialize the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:82: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "training_args = KETrainingArguments(\n",
    "    output_dir=\"../outputs\",\n",
    "    learning_rate=5e-5,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Initialize the model arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bloomberg/KBIR\"\n",
    "model_args = KEModelArguments(\n",
    "    model_name_or_path=model_name,\n",
    "    use_crf=True,\n",
    "    tokenizer_name=\"roberta-large\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/10/2022 06:19:17 - WARNING - dlkp.extraction.train_eval_kp_tagger -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "04/10/2022 06:19:17 - INFO - dlkp.extraction.train_eval_kp_tagger -   Training/evaluation parameters KETrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=1000,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../outputs/runs/Apr10_06-19-11_deb-research,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1000,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=../outputs,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../outputs,\n",
      "save_on_each_node=False,\n",
      "save_steps=1000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:344] 2022-04-10 06:19:17,435 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:648] 2022-04-10 06:19:17,568 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /home/debanjan/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "[INFO|configuration_utils.py:684] 2022-04-10 06:19:17,570 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-10 06:19:18,558 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /home/debanjan/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-10 06:19:18,559 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /home/debanjan/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-10 06:19:18,559 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /home/debanjan/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-10 06:19:18,559 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-10 06:19:18,560 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-10 06:19:18,560 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:648] 2022-04-10 06:19:18,686 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /home/debanjan/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "[INFO|configuration_utils.py:684] 2022-04-10 06:19:18,687 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/10/2022 06:19:19 - WARNING - datasets.builder -   Reusing dataset inspec (/home/debanjan/.cache/huggingface/datasets/midas___inspec/extraction/0.0.1/debd18641afb7048a36cee2b7bb8dfbf2cd1a68899118653a42fd760cf84284e)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1375.63it/s]\n",
      "#0:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\n",
      "#1:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "#2:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.23ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.19ba/s]\u001b[A\n",
      "\n",
      "\n",
      "#2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.54ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.24ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.27ba/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.21ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.70ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.12ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#0:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\n",
      "#1:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "#0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.67ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.05ba/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#4:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.28ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.05ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.67ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.01ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.00ba/s]\n",
      "#7: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.92ba/s]\n",
      "#0:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\n",
      "#1:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "#2:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.27ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.02ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.98ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.48ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.81ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:   0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.36ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#6: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.13ba/s]\n",
      "#7: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.19ba/s]\n",
      "[INFO|configuration_utils.py:648] 2022-04-10 06:19:21,571 >> loading configuration file https://huggingface.co/bloomberg/KBIR/resolve/main/config.json from cache at /home/debanjan/.cache/huggingface/transformers/e3e4e9cc0f5071082c8ddd8a31edf789b32dc26ba7dec62e3fbe88190bb22206.a2909b3b000ff4512970d210c555be4a09eeb3e22fbde068618b6919d78ddc34\n",
      "[INFO|configuration_utils.py:684] 2022-04-10 06:19:21,573 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"bloomberg/KBIR\",\n",
      "  \"architectures\": [\n",
      "    \"KLMForReplacementAndMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1431] 2022-04-10 06:19:21,733 >> loading weights file https://huggingface.co/bloomberg/KBIR/resolve/main/pytorch_model.bin from cache at /home/debanjan/.cache/huggingface/transformers/dcbcac674440cfdfe901cc2259b02eb80c5069676d931c4416a862656ebf0f49.5aec73b8a150e8021bbc6d6d1de4d9548566e457a9a5044790d0f685dcb48c6b\n",
      "[WARNING|modeling_utils.py:1693] 2022-04-10 06:19:23,810 >> Some weights of the model checkpoint at bloomberg/KBIR were not used when initializing RobertaForTokenClassification: ['infilling_head.mlp_layer_norm.layer_norm1.weight', 'infilling_head.mlp_layer_norm.linear1.weight', 'infilling_head.num_tok_classifier.bias', 'infilling_head.mlp_layer_norm.linear1.bias', 'infilling_head.position_embeddings.weight', 'replacement_classification_head.classifier.weight', 'lm_head.decoder.weight', 'infilling_head.mlp_layer_norm.layer_norm1.bias', 'lm_head.decoder.bias', 'infilling_head.num_tok_classifier.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'infilling_head.decoder.weight', 'infilling_head.bias', 'infilling_head.mlp_layer_norm.linear2.bias', 'lm_head.dense.bias', 'infilling_head.mlp_layer_norm.linear2.weight', 'lm_head.layer_norm.bias', 'infilling_head.mlp_layer_norm.layer_norm2.bias', 'replacement_classification_head.bias', 'infilling_head.mlp_layer_norm.layer_norm2.weight', 'replacement_classification_head.classifier.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1704] 2022-04-10 06:19:23,810 >> Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at bloomberg/KBIR and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|trainer.py:570] 2022-04-10 06:19:23,820 >> The following columns in the training set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: doc_bio_tags, document, id, special_tokens_mask. If doc_bio_tags, document, id, special_tokens_mask are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1279] 2022-04-10 06:19:23,828 >> ***** Running training *****\n",
      "[INFO|trainer.py:1280] 2022-04-10 06:19:23,829 >>   Num examples = 1000\n",
      "[INFO|trainer.py:1281] 2022-04-10 06:19:23,829 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1282] 2022-04-10 06:19:23,829 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1283] 2022-04-10 06:19:23,830 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:1284] 2022-04-10 06:19:23,830 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1285] 2022-04-10 06:19:23,830 >>   Total optimization steps = 250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/250 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mKeyphraseTagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/research/dlkp/src/dlkp/extraction/tagger.py:98\u001b[0m, in \u001b[0;36mKeyphraseTagger.train_and_eval\u001b[0;34m(model_args, data_args, training_args)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_eval\u001b[39m(model_args, data_args, training_args):\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_eval_extraction_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/research/dlkp/src/dlkp/extraction/train_eval_kp_tagger.py:165\u001b[0m, in \u001b[0;36mtrain_eval_extraction_model\u001b[0;34m(model_args, data_args, training_args)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()  \u001b[38;5;66;03m# Saves the tokenizer too for easy upload\u001b[39;00m\n\u001b[1;32m    168\u001b[0m output_train_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(training_args\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_results.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/trainer.py:1400\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1400\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1403\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1406\u001b[0m ):\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/trainer.py:2002\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2000\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2002\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/code/research/dlkp/venv/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/research/dlkp/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "KeyphraseTagger.train_and_eval(\n",
    "    model_args=model_args,\n",
    "    data_args=data_args,\n",
    "    training_args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Visualize your training progress using tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ../outputs/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Load the trained model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:646] 2022-04-10 06:20:31,931 >> loading configuration file ../outputs/config.json\n",
      "[INFO|configuration_utils.py:684] 2022-04-10 06:20:31,932 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../outputs\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"id_to_label\": {\n",
      "    \"0\": \"B\",\n",
      "    \"1\": \"I\",\n",
      "    \"2\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"label_to_id\": {\n",
      "    \"B\": 0,\n",
      "    \"I\": 1,\n",
      "    \"O\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"use_crf\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1703] 2022-04-10 06:20:31,933 >> Didn't find file ../outputs/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1784] 2022-04-10 06:20:31,933 >> loading file ../outputs/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1784] 2022-04-10 06:20:31,934 >> loading file ../outputs/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1784] 2022-04-10 06:20:31,934 >> loading file ../outputs/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1784] 2022-04-10 06:20:31,934 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1784] 2022-04-10 06:20:31,935 >> loading file ../outputs/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1784] 2022-04-10 06:20:31,935 >> loading file ../outputs/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1429] 2022-04-10 06:20:31,981 >> loading weights file ../outputs/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1702] 2022-04-10 06:20:33,916 >> All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "[INFO|modeling_utils.py:1710] 2022-04-10 06:20:33,917 >> All the weights of RobertaForTokenClassification were initialized from the model checkpoint at ../outputs.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
      "[INFO|trainer.py:299] 2022-04-10 06:20:33,919 >> No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "[INFO|training_args.py:1009] 2022-04-10 06:20:33,920 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:871] 2022-04-10 06:20:33,920 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "tagger = KeyphraseTagger.load(\n",
    "    model_name_or_path=\"../outputs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = (\n",
    "    \"In this work, we explore how to learn task-specific language models aimed towards learning rich \"\n",
    "    \"representation of keyphrases from text documents. We experiment with different masking strategies for \"\n",
    "    \"pre-training transformer language models (LMs) in discriminative as well as generative settings. In the \"\n",
    "    \"discriminative setting, we introduce a new pre-training objective - Keyphrase Boundary Infilling with \"\n",
    "    \"Replacement (KBIR), showing large gains in performance (upto 9.26 points in F1) over SOTA, when LM \"\n",
    "    \"pre-trained using KBIR is fine-tuned for the task of keyphrase extraction. In the generative setting, we \"\n",
    "    \"introduce a new pre-training setup for BART - KeyBART, that reproduces the keyphrases related to the \"\n",
    "    \"input text in the CatSeq format, instead of the denoised original input. This also led to gains in \"\n",
    "    \"performance (upto 4.33 points in F1@M) over SOTA for keyphrase generation. Additionally, we also \"\n",
    "    \"fine-tune the pre-trained language models on named entity recognition (NER), question answering (QA), \"\n",
    "    \"relation extraction (RE), abstractive summarization and achieve comparable performance with that of the \"\n",
    "    \"SOTA, showing that learning rich representation of keyphrases is indeed beneficial for many other \"\n",
    "    \"fundamental NLP tasks.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 539.46ex/s]\n",
      "[INFO|trainer.py:570] 2022-04-10 06:20:40,659 >> The following columns in the test set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: document, special_tokens_mask. If document, special_tokens_mask are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2389] 2022-04-10 06:20:40,661 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2391] 2022-04-10 06:20:40,661 >>   Num examples = 1\n",
      "[INFO|trainer.py:2394] 2022-04-10 06:20:40,662 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 203.77ex/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyph\n",
      "ases\n",
      "text documents\n",
      "masking strategies\n",
      "transformer language models\n",
      "gener\n",
      "Keyphrase Boundary Infilling\n",
      "keyphrase extraction\n",
      "KeyBART\n",
      "CatSeq\n",
      "key\n",
      "generation\n",
      "named entity recognition\n",
      "question answering\n",
      "relation extraction\n",
      "abstractive summarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "keyphrases = tagger.predict(input_text)\n",
    "for keyphrase in keyphrases[0]:\n",
    "    print(keyphrase.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a finetuned model from Huggingface for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:648] 2022-04-10 06:20:57,189 >> loading configuration file https://huggingface.co/midas/roberta-large-inspec-finetuned-crf/resolve/main/config.json from cache at /home/debanjan/.cache/huggingface/transformers/f1f126916a7a2fb794a7e4693bfcb54b5e3afa12914f4751d03e9e6aeaf8f327.7f5aaad40bd4685451ce4d5715f045f19bd668b52b902280ec29b3e9cd5d7f96\n",
      "[INFO|configuration_utils.py:684] 2022-04-10 06:20:57,191 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"midas/roberta-large-inspec-finetuned-crf\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"id_to_label\": {\n",
      "    \"0\": \"B\",\n",
      "    \"1\": \"I\",\n",
      "    \"2\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"label_to_id\": {\n",
      "    \"B\": 0,\n",
      "    \"I\": 1,\n",
      "    \"O\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"use_crf\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-10 06:20:58,351 >> loading file https://huggingface.co/midas/roberta-large-inspec-finetuned-crf/resolve/main/vocab.json from cache at /home/debanjan/.cache/huggingface/transformers/629fcccae7664370fd5041de46b899555b12896bd0b82c7d96e6f0bc6dd89f1b.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-10 06:20:58,351 >> loading file https://huggingface.co/midas/roberta-large-inspec-finetuned-crf/resolve/main/merges.txt from cache at /home/debanjan/.cache/huggingface/transformers/9f8dd45f90afcae6555efd129aefcf8b81622652f6e605141197f427120267f6.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-10 06:20:58,352 >> loading file https://huggingface.co/midas/roberta-large-inspec-finetuned-crf/resolve/main/tokenizer.json from cache at /home/debanjan/.cache/huggingface/transformers/992c6e3d6170015a3af820c97d2aff54744269a67fe409a01623a0efcb9cbfd4.ce08456968edb9e6fa18191f68e038a6e3d941ff70678790aa9c2ff0881e157d\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-10 06:20:58,352 >> loading file https://huggingface.co/midas/roberta-large-inspec-finetuned-crf/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-10 06:20:58,353 >> loading file https://huggingface.co/midas/roberta-large-inspec-finetuned-crf/resolve/main/special_tokens_map.json from cache at /home/debanjan/.cache/huggingface/transformers/07dc4adb9740a76606e86f5221f087c6f4f5e6003fb3a78fcd747d9bdfb71079.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-10 06:20:58,353 >> loading file https://huggingface.co/midas/roberta-large-inspec-finetuned-crf/resolve/main/tokenizer_config.json from cache at /home/debanjan/.cache/huggingface/transformers/9fb2b06007773bcb94b07f249d74a8613c897517106b1245ff03aff49036e612.3801b48271f33c04c79c88694f9a4604eb685e3d0630d02ad63c67a6802c1f44\n",
      "[INFO|modeling_utils.py:1431] 2022-04-10 06:20:58,538 >> loading weights file https://huggingface.co/midas/roberta-large-inspec-finetuned-crf/resolve/main/pytorch_model.bin from cache at /home/debanjan/.cache/huggingface/transformers/206aacdecd6567b5c698b79dcad606472a8120f97023ad2f1dafff3ceef66dbd.becbffa0950aa8cb1e8f011a892ee282bc7042847a86586c8eafebf886216eec\n",
      "[INFO|modeling_utils.py:1702] 2022-04-10 06:21:00,499 >> All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "[INFO|modeling_utils.py:1710] 2022-04-10 06:21:00,500 >> All the weights of RobertaForTokenClassification were initialized from the model checkpoint at midas/roberta-large-inspec-finetuned-crf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
      "[INFO|trainer.py:299] 2022-04-10 06:21:00,502 >> No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "[INFO|training_args.py:1009] 2022-04-10 06:21:00,503 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:871] 2022-04-10 06:21:00,503 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "tagger = KeyphraseTagger.load(\n",
    "    model_name_or_path=\"midas/roberta-large-inspec-finetuned-crf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 536.22ex/s]\n",
      "[INFO|trainer.py:570] 2022-04-10 06:21:06,939 >> The following columns in the test set  don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: document, special_tokens_mask. If document, special_tokens_mask are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2389] 2022-04-10 06:21:06,940 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2391] 2022-04-10 06:21:06,941 >>   Num examples = 1\n",
      "[INFO|trainer.py:2394] 2022-04-10 06:21:06,942 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 203.49ex/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rich representation\n",
      "text documents\n",
      "masking strategies\n",
      "transformer language models\n",
      "Keyphrase Boundary Infilling with Replacement\n",
      "KBIR\n",
      "KBIR\n",
      "KeyBART\n",
      "CatSeq\n",
      "named entity recognition\n",
      "question answering\n",
      "relation extraction\n",
      "abstractive summarization\n",
      "rich representation\n",
      "NLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "keyphrases = tagger.predict(input_text)\n",
    "for keyphrase in keyphrases[0]:\n",
    "    print(keyphrase.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
