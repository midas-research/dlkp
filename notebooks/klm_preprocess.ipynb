{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "klm_preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEHkPwSWLBBD",
        "outputId": "2adf486e-ff7b-4750-ec88-b8d0933f951d"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSvgoduMLGEp"
      },
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "from transformers import RobertaTokenizer, PreTrainedTokenizer\n",
        "from transformers import RobertaConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoVheMQrnEmb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3oHJencLGMb"
      },
      "source": [
        "#config\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")\n",
        "\n",
        "#model roberta\n",
        "model = RobertaForMaskedLM(config=config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yxtwm9xMYN3"
      },
      "source": [
        "**Code requirement**\n",
        "\n",
        "\n",
        "1.   Dataset class :\n",
        "\n",
        "        load and tokenize dataset->> input ids\n",
        "\n",
        "        *look if nlp dataset library could be used here easily*\n",
        "\n",
        "        tokenize key phrase as well as text and mask key phrase in data collator\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "2.   Data collator for masked LM\n",
        "\n",
        "    takes a list of samples from a Dataset and collate them into a batch for (also masking and stuffs)\n",
        "\n",
        "    Refrence class: DataCollatorForWholeWordMask\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kKlX2aRM0k2"
      },
      "source": [
        "**Dataset class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vE_U_IuLGP9"
      },
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "import json, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2CG9HfpNmLs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWl2QW0TNmRE"
      },
      "source": [
        "class KLMDataset(Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\n",
        "        assert os.path.isfile(file_path)\n",
        "\n",
        "        # logger.info(\"Creating features from dataset file at %s\", file_path)\n",
        "        self.abst= []\n",
        "        self.kps= []\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                d=json.loads(line)\n",
        "                self.abst.append(d['text'])\n",
        "                self.kps.append(d['kp'])\n",
        "\n",
        "        for (i,kp) in enumerate(self.kps):\n",
        "            self.kps[i]= tokenizer(kp,add_special_tokens= False, truncation= False)['input_ids']\n",
        " \n",
        "\n",
        "        self.abst = tokenizer(self.abst, add_special_tokens=True, truncation=True, max_length=block_size)[\"input_ids\"]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.abst)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # print(\"called {} and results{}\\n\".format(i,{'input_ids': self.abst[i], 'kp': self.kps[i]}))\n",
        "        return {'input_ids': self.abst[i], 'kp': self.kps[i]}\n",
        "\n",
        "# super daset from HF\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs-wm7RVNmUP"
      },
      "source": [
        "tok= RobertaTokenizer.from_pretrained(\"roberta-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "991fGSu8hW-n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gLTyA_0hvMU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIXBbF_ZYqCF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5NOBJhfNmXQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_GyWRwqM5KI"
      },
      "source": [
        "**Data collator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5FcKIbkLGS2"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
        "import torch\n",
        "from transformers.data.data_collator import _collate_batch, tolist\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZMjWP8mLGVz"
      },
      "source": [
        "@dataclass\n",
        "class DataCollatorForKLM(DataCollatorForLanguageModeling):\n",
        "    def __init__(self, \n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        mlm_probability= 0.15,\n",
        "        kp_mask_percentage = 0.8):\n",
        "        self.tokenizer= tokenizer\n",
        "        self.mlm_probability= mlm_probability\n",
        "        self.kp_mask_percentage = kp_mask_percentage\n",
        "\n",
        "    def __call__(\n",
        "        self, examples\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        print(\"collator   \",examples)\n",
        "        if isinstance(examples[0], dict):\n",
        "            print(examples[0])\n",
        "            input_ids = [e[\"input_ids\"] for e in examples]\n",
        "            key_phrases= [e[\"labels\"] for e in examples]\n",
        "        else:\n",
        "          print(\"proper inputr fromat is not found for kp input ids\")\n",
        "  \n",
        "\n",
        "        batch_input = _collate_batch(input_ids, self.tokenizer)\n",
        "\n",
        "        mask_labels = []\n",
        "        kp_mask_labels= []\n",
        "        for e in examples:\n",
        "            ref_tokens = []\n",
        "            kp_tokens_list= []\n",
        "            for id in tolist(e[\"input_ids\"]):\n",
        "                token = self.tokenizer._convert_id_to_token(id)\n",
        "                ref_tokens.append(token)\n",
        "            for kp in tolist(e[\"labels\"]):\n",
        "                curr_kp= []\n",
        "                for kp_id in kp:\n",
        "                    tok= self.tokenizer._convert_id_to_token(kp_id)\n",
        "                    curr_kp.append(tok)\n",
        "                if len(curr_kp) >0:\n",
        "                    kp_tokens_list.append(curr_kp)\n",
        "            mask_res= self.kp_and_whole_word_mask(ref_tokens, kp_tokens_list) #[[\"KP1-T1\", \"KP1-T2\"], [\"KP2-T1\", \"KP2-T2\", \"KP2-T3\"]] \n",
        "            mask_labels.append(mask_res[0])\n",
        "            kp_mask_labels.append(mask_res[1])\n",
        "        #collate\n",
        "        batch_mask = _collate_batch(mask_labels, self.tokenizer)\n",
        "        kp_batch_mask= _collate_batch(kp_mask_labels, self.tokenizer)\n",
        "        #mask\n",
        "        inputs, labels = self.mask_tokens_and_kp(batch_input, batch_mask, kp_batch_mask)\n",
        "\n",
        "        return {\"input_ids\": inputs, \"labels\": labels}\n",
        "\n",
        "    def kp_and_whole_word_mask(self, input_tokens, kp_tokens_list, max_predictions=512):\n",
        "        \"\"\"\n",
        "        Get 0/1 labels for masked tokens with whole word mask proxy\n",
        "        \"\"\"\n",
        "\n",
        "        cand_indexes = []\n",
        "        kp_indexes= []\n",
        "        for (i, token) in enumerate(input_tokens):\n",
        "            if token == \"[CLS]\" or token == \"[SEP]\":\n",
        "                continue\n",
        "            kp_flag = False\n",
        "            for kp in kp_tokens_list: # kp = [\"KP1-T1\", \"KP1-T2\"]\n",
        "                j= i + len(kp)\n",
        "                if j < len(input_tokens):\n",
        "                    if input_tokens[i:j]== kp: # input_tokens = [\"KP1-T1\", \"KP1-T2\"]\n",
        "                      kp_indexes.append([x for x in range(i,j)]) # kp_indexes = [\"index of KP1-T1\", \"index of KP1-T2\"]\n",
        "                      i=j-1\n",
        "                      kp_flag= True\n",
        "                      break\n",
        "            if kp_flag: #if token is included in kp mask then don't include in random token mask\n",
        "                continue\n",
        "            if len(cand_indexes) >= 1 and token.startswith(\"##\"):\n",
        "                cand_indexes[-1].append(i)\n",
        "            else:\n",
        "                cand_indexes.append([i])\n",
        "            \n",
        "        tok_to_predict= min(max_predictions, max(1, int(round(len(input_tokens) * self.mlm_probability))))\n",
        "        kp_to_predict= min(max_predictions, max(1, int(round(len(kp_tokens_list) * self.kp_mask_percentage))))\n",
        "\n",
        "        tok_mask_labels= self.get_mask_labels(cand_indexes=cand_indexes, len_input_tokens=len(input_tokens), num_to_predict=tok_to_predict)\n",
        "        kp_mask_labels= self.get_mask_labels(cand_indexes=kp_indexes, len_input_tokens=len(input_tokens), num_to_predict=kp_to_predict)\n",
        "        return tok_mask_labels, kp_mask_labels\n",
        "\n",
        "\n",
        "    def get_mask_labels(self, cand_indexes, len_input_tokens, num_to_predict):\n",
        "        random.shuffle(cand_indexes)\n",
        "        masked_lms = []\n",
        "        covered_indexes = set()\n",
        "        for index_set in cand_indexes:\n",
        "            if len(masked_lms) >= num_to_predict:\n",
        "                break\n",
        "            # If adding a whole-word mask would exceed the maximum number of\n",
        "            # predictions, then just skip this candidate.\n",
        "            if len(masked_lms) + len(index_set) > num_to_predict:\n",
        "                continue\n",
        "            is_any_index_covered = False\n",
        "            for index in index_set:\n",
        "                if index in covered_indexes:\n",
        "                    is_any_index_covered = True\n",
        "                    break\n",
        "            if is_any_index_covered:\n",
        "                continue\n",
        "            for index in index_set:\n",
        "                covered_indexes.add(index)\n",
        "                masked_lms.append(index)\n",
        "\n",
        "        assert len(covered_indexes) == len(masked_lms)\n",
        "        mask_labels = [1 if i in covered_indexes else 0 for i in range(len_input_tokens)]\n",
        "        return mask_labels\n",
        "\n",
        "    def mask_tokens_and_kp(self, inputs, mask_labels, kp_mask_labels): \n",
        "        \"\"\"\n",
        "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n",
        "        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.tokenizer.mask_token is None:\n",
        "            raise ValueError(\n",
        "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n",
        "            )\n",
        "        labels = inputs.clone()\n",
        "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "\n",
        "        probability_matrix = mask_labels\n",
        "        kp_probability_matrix = kp_mask_labels\n",
        "\n",
        "        special_tokens_mask = [\n",
        "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "        ]\n",
        "        # do zero for special tokens\n",
        "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "        kp_probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "\n",
        "        # assert kp_probability_matrix & probability_matrix == 0\n",
        "        # do zero for padded points\n",
        "        if self.tokenizer._pad_token is not None:\n",
        "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
        "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
        "            kp_probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
        "\n",
        "        masked_indices = probability_matrix.bool()\n",
        "        kp_masked_indices = kp_probability_matrix.bool()\n",
        "        # get the gold lables\n",
        "        labels[~(masked_indices | kp_masked_indices)] = -100  # We only compute loss on random masked tokens and kp masked token else is set to -100\n",
        "\n",
        "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "        # 80 % masking for key phrases\n",
        "        kp_indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & kp_masked_indices\n",
        "        inputs[kp_indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "        # generate random tokens\n",
        "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
        "        # 10% of the time, we replace masked input tokens with random word\n",
        "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "        inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "        # replace 10 # kp tokens with random idices\n",
        "        kp_indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & kp_masked_indices & ~kp_indices_replaced\n",
        "        inputs[kp_indices_random] = random_words[kp_indices_random]\n",
        "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "        # print(\"inside mask tok functiom \\n\",inputs,\"\\n\", labels,\"\\n\")\n",
        "\n",
        "        # generation - t1, t2, t3 (actual) - [MASK], t4 [MASK], t5, t6\n",
        "        # replacement - t1, t2, t3 (actual) - [MASK], t4 [MASK], t5, t6 (replace) t9\n",
        "        \n",
        "        return inputs, labels\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9azrJU6UuPFt"
      },
      "source": [
        "from datasets import load_dataset\n",
        "def load_klm_dataset(tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\n",
        "    \n",
        "    def pre_process(d):\n",
        "        kp_pro= tokenizer(d['kp'],add_special_tokens= False, truncation= False)[\"input_ids\"]\n",
        "        d['input_ids']= tokenizer(d['text'], add_special_tokens=True, truncation=True, max_length=block_size)[\"input_ids\"]\n",
        "        d['labels'] = kp_pro\n",
        "        # print(\"inn inn\",d['kp'])\n",
        "        return d\n",
        "\n",
        "\n",
        "    dataset = load_dataset('json', data_files= file_path, split='train' )\n",
        "    dataset= dataset.map(pre_process)\n",
        "    # print(\"inn \", dataset)\n",
        "    dataset.set_format(columns=[ 'labels', 'input_ids'])\n",
        "\n",
        "    return dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d2Zn2PJvQhl"
      },
      "source": [
        "#  tok(['iam mam'],add_special_tokens= False, truncation= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa4ArAI0kbs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc7ec82-6a79-416b-83c4-a9bf904e2536"
      },
      "source": [
        "\n",
        "# data_set= KLMDataset(tokenizer=tok, file_path=\"/content/dummy.txt\", block_size= 200)\n",
        "data_set = load_klm_dataset(tokenizer= tok, file_path= \"/content/train.json\", block_size= 124)\n",
        "# data_set.set_format(columns=[ 'kp', 'input_ids'])\n",
        "dc= DataCollatorForKLM(tokenizer= tok)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset json (/content/json/default-16dd99a81353c724/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514)\n",
            "Loading cached processed dataset at /content/json/default-16dd99a81353c724/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514/cache-6d89745262ae7664.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEo-CRzkLGYQ"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_gpu_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2, # need to save all the models\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rvp2Yk5LGa7"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=dc,\n",
        "    train_dataset= data_set\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv-vnqziLGda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "21fccf71-22e9-4403-b8e0-2bd1f86d60fa"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "collator    [{'input_ids': [0, 574, 23259, 2012, 19, 80, 82, 27744, 301, 8, 744, 8, 51, 2845, 7, 310, 784, 23259, 4, 20, 177, 51, 310, 44555, 7, 5, 1074, 9, 208, 2611, 257, 5371, 1439, 2636, 6, 3773, 1671, 6, 726, 257, 8, 163, 2582, 257, 4, 2], 'labels': [[7109, 82], [5367, 8, 744], [7215, 5982, 1626]]}, {'input_ids': [0, 33282, 1671, 8, 840, 338, 18078, 4620, 5, 5718, 526, 4, 252, 465, 14, 951, 34, 2673, 10, 569, 9, 106, 519, 2099, 8, 33, 17199, 24, 7, 5, 3742, 4, 1892, 51, 386, 5, 1015, 7, 465, 5, 17685, 8, 185, 159, 5, 569, 137, 840, 338, 18078, 18, 74, 12, 1610, 1623, 5684, 66, 4, 2], 'labels': [[34103, 526], [12326, 7, 465, 5, 17685]]}, {'input_ids': [0, 574, 23259, 2012, 19, 80, 82, 27744, 301, 8, 744, 8, 51, 2845, 7, 310, 784, 23259, 4, 20, 177, 51, 310, 44555, 7, 5, 1074, 9, 208, 2611, 257, 5371, 1439, 2636, 6, 3773, 1671, 6, 726, 257, 8, 163, 2582, 257, 4, 2], 'labels': [[7109, 82], [5367, 8, 744], [7215, 5982, 1626]]}, {'input_ids': [0, 33282, 1671, 8, 840, 338, 18078, 4620, 5, 5718, 526, 4, 252, 465, 14, 951, 34, 2673, 10, 569, 9, 106, 519, 2099, 8, 33, 17199, 24, 7, 5, 3742, 4, 1892, 51, 386, 5, 1015, 7, 465, 5, 17685, 8, 185, 159, 5, 569, 137, 840, 338, 18078, 18, 74, 12, 1610, 1623, 5684, 66, 4, 2], 'labels': [[34103, 526], [12326, 7, 465, 5, 17685]]}, {'input_ids': [0, 574, 23259, 2012, 19, 80, 82, 27744, 301, 8, 744, 8, 51, 2845, 7, 310, 784, 23259, 4, 20, 177, 51, 310, 44555, 7, 5, 1074, 9, 208, 2611, 257, 5371, 1439, 2636, 6, 3773, 1671, 6, 726, 257, 8, 163, 2582, 257, 4, 2], 'labels': [[7109, 82], [5367, 8, 744], [7215, 5982, 1626]]}, {'input_ids': [0, 574, 23259, 2012, 19, 80, 82, 27744, 301, 8, 744, 8, 51, 2845, 7, 310, 784, 23259, 4, 20, 177, 51, 310, 44555, 7, 5, 1074, 9, 208, 2611, 257, 5371, 1439, 2636, 6, 3773, 1671, 6, 726, 257, 8, 163, 2582, 257, 4, 2], 'labels': [[7109, 82], [5367, 8, 744], [7215, 5982, 1626]]}, {'input_ids': [0, 574, 23259, 2012, 19, 80, 82, 27744, 301, 8, 744, 8, 51, 2845, 7, 310, 784, 23259, 4, 20, 177, 51, 310, 44555, 7, 5, 1074, 9, 208, 2611, 257, 5371, 1439, 2636, 6, 3773, 1671, 6, 726, 257, 8, 163, 2582, 257, 4, 2], 'labels': [[7109, 82], [5367, 8, 744], [7215, 5982, 1626]]}, {'input_ids': [0, 33282, 1671, 8, 840, 338, 18078, 4620, 5, 5718, 526, 4, 252, 465, 14, 951, 34, 2673, 10, 569, 9, 106, 519, 2099, 8, 33, 17199, 24, 7, 5, 3742, 4, 1892, 51, 386, 5, 1015, 7, 465, 5, 17685, 8, 185, 159, 5, 569, 137, 840, 338, 18078, 18, 74, 12, 1610, 1623, 5684, 66, 4, 2], 'labels': [[34103, 526], [12326, 7, 465, 5, 17685]]}, {'input_ids': [0, 574, 23259, 2012, 19, 80, 82, 27744, 301, 8, 744, 8, 51, 2845, 7, 310, 784, 23259, 4, 20, 177, 51, 310, 44555, 7, 5, 1074, 9, 208, 2611, 257, 5371, 1439, 2636, 6, 3773, 1671, 6, 726, 257, 8, 163, 2582, 257, 4, 2], 'labels': [[7109, 82], [5367, 8, 744], [7215, 5982, 1626]]}, {'input_ids': [0, 33282, 1671, 8, 840, 338, 18078, 4620, 5, 5718, 526, 4, 252, 465, 14, 951, 34, 2673, 10, 569, 9, 106, 519, 2099, 8, 33, 17199, 24, 7, 5, 3742, 4, 1892, 51, 386, 5, 1015, 7, 465, 5, 17685, 8, 185, 159, 5, 569, 137, 840, 338, 18078, 18, 74, 12, 1610, 1623, 5684, 66, 4, 2], 'labels': [[34103, 526], [12326, 7, 465, 5, 17685]]}, {'input_ids': [0, 33282, 1671, 8, 840, 338, 18078, 4620, 5, 5718, 526, 4, 252, 465, 14, 951, 34, 2673, 10, 569, 9, 106, 519, 2099, 8, 33, 17199, 24, 7, 5, 3742, 4, 1892, 51, 386, 5, 1015, 7, 465, 5, 17685, 8, 185, 159, 5, 569, 137, 840, 338, 18078, 18, 74, 12, 1610, 1623, 5684, 66, 4, 2], 'labels': [[34103, 526], [12326, 7, 465, 5, 17685]]}, {'input_ids': [0, 33282, 1671, 8, 840, 338, 18078, 4620, 5, 5718, 526, 4, 252, 465, 14, 951, 34, 2673, 10, 569, 9, 106, 519, 2099, 8, 33, 17199, 24, 7, 5, 3742, 4, 1892, 51, 386, 5, 1015, 7, 465, 5, 17685, 8, 185, 159, 5, 569, 137, 840, 338, 18078, 18, 74, 12, 1610, 1623, 5684, 66, 4, 2], 'labels': [[34103, 526], [12326, 7, 465, 5, 17685]]}, {'input_ids': [0, 574, 23259, 2012, 19, 80, 82, 27744, 301, 8, 744, 8, 51, 2845, 7, 310, 784, 23259, 4, 20, 177, 51, 310, 44555, 7, 5, 1074, 9, 208, 2611, 257, 5371, 1439, 2636, 6, 3773, 1671, 6, 726, 257, 8, 163, 2582, 257, 4, 2], 'labels': [[7109, 82], [5367, 8, 744], [7215, 5982, 1626]]}, {'input_ids': [0, 33282, 1671, 8, 840, 338, 18078, 4620, 5, 5718, 526, 4, 252, 465, 14, 951, 34, 2673, 10, 569, 9, 106, 519, 2099, 8, 33, 17199, 24, 7, 5, 3742, 4, 1892, 51, 386, 5, 1015, 7, 465, 5, 17685, 8, 185, 159, 5, 569, 137, 840, 338, 18078, 18, 74, 12, 1610, 1623, 5684, 66, 4, 2], 'labels': [[34103, 526], [12326, 7, 465, 5, 17685]]}, {'input_ids': [0, 33282, 1671, 8, 840, 338, 18078, 4620, 5, 5718, 526, 4, 252, 465, 14, 951, 34, 2673, 10, 569, 9, 106, 519, 2099, 8, 33, 17199, 24, 7, 5, 3742, 4, 1892, 51, 386, 5, 1015, 7, 465, 5, 17685, 8, 185, 159, 5, 569, 137, 840, 338, 18078, 18, 74, 12, 1610, 1623, 5684, 66, 4, 2], 'labels': [[34103, 526], [12326, 7, 465, 5, 17685]]}, {'input_ids': [0, 574, 23259, 2012, 19, 80, 82, 27744, 301, 8, 744, 8, 51, 2845, 7, 310, 784, 23259, 4, 20, 177, 51, 310, 44555, 7, 5, 1074, 9, 208, 2611, 257, 5371, 1439, 2636, 6, 3773, 1671, 6, 726, 257, 8, 163, 2582, 257, 4, 2], 'labels': [[7109, 82], [5367, 8, 744], [7215, 5982, 1626]]}, {'input_ids': [0, 574, 23259, 2012, 19, 80, 82, 27744, 301, 8, 744, 8, 51, 2845, 7, 310, 784, 23259, 4, 20, 177, 51, 310, 44555, 7, 5, 1074, 9, 208, 2611, 257, 5371, 1439, 2636, 6, 3773, 1671, 6, 726, 257, 8, 163, 2582, 257, 4, 2], 'labels': [[7109, 82], [5367, 8, 744], [7215, 5982, 1626]]}, {'input_ids': [0, 33282, 1671, 8, 840, 338, 18078, 4620, 5, 5718, 526, 4, 252, 465, 14, 951, 34, 2673, 10, 569, 9, 106, 519, 2099, 8, 33, 17199, 24, 7, 5, 3742, 4, 1892, 51, 386, 5, 1015, 7, 465, 5, 17685, 8, 185, 159, 5, 569, 137, 840, 338, 18078, 18, 74, 12, 1610, 1623, 5684, 66, 4, 2], 'labels': [[34103, 526], [12326, 7, 465, 5, 17685]]}]\n",
            "{'input_ids': [0, 574, 23259, 2012, 19, 80, 82, 27744, 301, 8, 744, 8, 51, 2845, 7, 310, 784, 23259, 4, 20, 177, 51, 310, 44555, 7, 5, 1074, 9, 208, 2611, 257, 5371, 1439, 2636, 6, 3773, 1671, 6, 726, 257, 8, 163, 2582, 257, 4, 2], 'labels': [[7109, 82], [5367, 8, 744], [7215, 5982, 1626]]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1, training_loss=10.954537391662598)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDsc-Ld-LGgM"
      },
      "source": [
        "# Data format - {\"text\": ....., \"keyphrases\": [{\"surface_form\": ..., \"start\": ..., \"end\": ...}]}\n",
        "# format - jsonl one json per line\n",
        "# dir - 1.jsonl, 2.jsonl, 3.jsonl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncOg07xPLGkN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC3T7Y35LGnQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}