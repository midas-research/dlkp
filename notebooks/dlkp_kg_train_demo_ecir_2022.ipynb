{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dlkp.models import KeyphraseGenerator\n",
    "from dlkp.generation import KGTrainingArguments, KGModelArguments, KGDataArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = KGDataArguments(\n",
    "    dataset_name=\"midas/inspec\",\n",
    "    dataset_config_name=\"generation\",\n",
    "    text_column_name=\"document\",\n",
    "    keyphrases_column_name=\"extractive_keyphrases\",\n",
    "    n_best_size=5,\n",
    "    num_beams=3,\n",
    "    cat_sequence=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = KGTrainingArguments(\n",
    "    output_dir=\"../outputs/generation\",\n",
    "    predict_with_generate=True,\n",
    "    learning_rate=4e-5,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=False,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = KGModelArguments(model_name_or_path=\"bloomberg/KeyBART\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/09/2022 18:55:01 - WARNING - dlkp.generation.train_eval_generator - Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: False\n",
      "04/09/2022 18:55:01 - INFO - dlkp.generation.train_eval_generator - Training/evaluation parameters KGTrainingArguments(\n",
      "_n_gpu=2,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=1000,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=4e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../outputs/generation/runs/Apr09_18-54-30_deb-research,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1000,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_return_sequences=None,\n",
      "num_train_epochs=50,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=../outputs/generation,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../outputs/generation,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:648] 2022-04-09 18:55:01,522 >> loading configuration file https://huggingface.co/bloomberg/KeyBART/resolve/main/config.json from cache at /home/debanjan/.cache/huggingface/transformers/d3c4f4b89efa42c978f3ea87e7a63b0a1bdfb0184bf17c014ef60c59c93b7da3.9458ab4fca85981d74f760077eca0099f9dac8306a47aaf172020a5397a4c223\n",
      "[INFO|configuration_utils.py:684] 2022-04-09 18:55:01,525 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"bloomberg/KeyBART\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:648] 2022-04-09 18:55:01,779 >> loading configuration file https://huggingface.co/bloomberg/KeyBART/resolve/main/config.json from cache at /home/debanjan/.cache/huggingface/transformers/d3c4f4b89efa42c978f3ea87e7a63b0a1bdfb0184bf17c014ef60c59c93b7da3.9458ab4fca85981d74f760077eca0099f9dac8306a47aaf172020a5397a4c223\n",
      "[INFO|configuration_utils.py:684] 2022-04-09 18:55:01,780 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"bloomberg/KeyBART\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-09 18:55:02,798 >> loading file https://huggingface.co/bloomberg/KeyBART/resolve/main/vocab.json from cache at /home/debanjan/.cache/huggingface/transformers/0dca1f21b6b7dd81ec2f6ccaa585bb948468b9d659d7e1f7ef736b0257a52ef3.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-09 18:55:02,798 >> loading file https://huggingface.co/bloomberg/KeyBART/resolve/main/merges.txt from cache at /home/debanjan/.cache/huggingface/transformers/02391494b5a80f26c9377f89b821f84967c7ee1044eb0d0f1a4a563d861adee1.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-09 18:55:02,799 >> loading file https://huggingface.co/bloomberg/KeyBART/resolve/main/tokenizer.json from cache at /home/debanjan/.cache/huggingface/transformers/ba4aa29d3681f0206019996d9760698dc3c804298511bf103d26addda5d83ce2.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-09 18:55:02,799 >> loading file https://huggingface.co/bloomberg/KeyBART/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-09 18:55:02,799 >> loading file https://huggingface.co/bloomberg/KeyBART/resolve/main/special_tokens_map.json from cache at /home/debanjan/.cache/huggingface/transformers/e5e6feabb046cfb44e5587cc225d0b43fe27a2874392ca9ddaeb7126e7536592.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n",
      "[INFO|tokenization_utils_base.py:1786] 2022-04-09 18:55:02,800 >> loading file https://huggingface.co/bloomberg/KeyBART/resolve/main/tokenizer_config.json from cache at /home/debanjan/.cache/huggingface/transformers/936201a20b400f95d08e028d77e7eb832db435a59e841f160a6137614223dfd6.56bca6f9c0179730b61060b9f77d8babfe41b52effa264d561c8ddc7fb3f912e\n",
      "[INFO|configuration_utils.py:648] 2022-04-09 18:55:02,944 >> loading configuration file https://huggingface.co/bloomberg/KeyBART/resolve/main/config.json from cache at /home/debanjan/.cache/huggingface/transformers/d3c4f4b89efa42c978f3ea87e7a63b0a1bdfb0184bf17c014ef60c59c93b7da3.9458ab4fca85981d74f760077eca0099f9dac8306a47aaf172020a5397a4c223\n",
      "[INFO|configuration_utils.py:684] 2022-04-09 18:55:02,945 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"bloomberg/KeyBART\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/09/2022 18:55:03 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/09/2022 18:55:03 - INFO - datasets.info - Loading Dataset info from /home/debanjan/.cache/huggingface/datasets/midas___inspec/generation/0.0.1/debd18641afb7048a36cee2b7bb8dfbf2cd1a68899118653a42fd760cf84284e\n",
      "04/09/2022 18:55:03 - WARNING - datasets.builder - Reusing dataset inspec (/home/debanjan/.cache/huggingface/datasets/midas___inspec/generation/0.0.1/debd18641afb7048a36cee2b7bb8dfbf2cd1a68899118653a42fd760cf84284e)\n",
      "04/09/2022 18:55:03 - INFO - datasets.info - Loading Dataset info from /home/debanjan/.cache/huggingface/datasets/midas___inspec/generation/0.0.1/debd18641afb7048a36cee2b7bb8dfbf2cd1a68899118653a42fd760cf84284e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 414.33it/s]\n",
      "  0%|                                                                                                                                                                                                    | 0/1000 [00:00<?, ?ex/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/09/2022 18:55:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/debanjan/.cache/huggingface/datasets/midas___inspec/generation/0.0.1/debd18641afb7048a36cee2b7bb8dfbf2cd1a68899118653a42fd760cf84284e/cache-5ad159c1e36ee61e.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 516.27ex/s]\n",
      "  0%|                                                                                                                                                                                                     | 0/500 [00:00<?, ?ex/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/09/2022 18:55:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/debanjan/.cache/huggingface/datasets/midas___inspec/generation/0.0.1/debd18641afb7048a36cee2b7bb8dfbf2cd1a68899118653a42fd760cf84284e/cache-ec74da9311830ddd.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 560.48ex/s]\n",
      "  0%|                                                                                                                                                                                                     | 0/500 [00:00<?, ?ex/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/09/2022 18:55:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/debanjan/.cache/huggingface/datasets/midas___inspec/generation/0.0.1/debd18641afb7048a36cee2b7bb8dfbf2cd1a68899118653a42fd760cf84284e/cache-dfcf29c4ee2ab17b.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 560.19ex/s]\n",
      "[INFO|modeling_utils.py:1431] 2022-04-09 18:55:07,803 >> loading weights file https://huggingface.co/bloomberg/KeyBART/resolve/main/pytorch_model.bin from cache at /home/debanjan/.cache/huggingface/transformers/277d84cd7ca8e4aea34cc45faf3976559f510c572eadb4c06d6428fa4bdf10d2.5fd0da0c4c8b79f9789db1511826fa743a04bf996360f94176176478d0a57b5d\n",
      "[INFO|modeling_utils.py:1702] 2022-04-09 18:55:10,769 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1710] 2022-04-09 18:55:10,770 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at bloomberg/KeyBART.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "[INFO|trainer.py:570] 2022-04-09 18:55:14,705 >> The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, document, extractive_keyphrases, abstractive_keyphrases. If id, document, extractive_keyphrases, abstractive_keyphrases are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1279] 2022-04-09 18:55:14,753 >> ***** Running training *****\n",
      "[INFO|trainer.py:1280] 2022-04-09 18:55:14,754 >>   Num examples = 1000\n",
      "[INFO|trainer.py:1281] 2022-04-09 18:55:14,754 >>   Num Epochs = 50\n",
      "[INFO|trainer.py:1282] 2022-04-09 18:55:14,754 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1283] 2022-04-09 18:55:14,755 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:1284] 2022-04-09 18:55:14,755 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1285] 2022-04-09 18:55:14,755 >>   Total optimization steps = 3150\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\", line 1329, in forward\n    outputs = self.model(\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\", line 1198, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\", line 824, in forward\n    layer_outputs = encoder_layer(\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\", line 307, in forward\n    hidden_states, attn_weights, _ = self.self_attn(\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\", line 215, in forward\n    attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\nRuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 23.69 GiB total capacity; 6.76 GiB already allocated; 75.56 MiB free; 6.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mKeyphraseGenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/research/dlkp/src/dlkp/generation/generator.py:93\u001b[0m, in \u001b[0;36mKeyphraseGenerator.train_and_eval\u001b[0;34m(model_args, data_args, training_args)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_eval\u001b[39m(model_args, data_args, training_args):\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_and_eval_generation_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/research/dlkp/src/dlkp/generation/train_eval_generator.py:191\u001b[0m, in \u001b[0;36mtrain_and_eval_generation_model\u001b[0;34m(model_args, data_args, training_args)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m last_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m last_checkpoint\n\u001b[0;32m--> 191\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()  \u001b[38;5;66;03m# Saves the tokenizer too for easy upload\u001b[39;00m\n\u001b[1;32m    193\u001b[0m metrics \u001b[38;5;241m=\u001b[39m train_result\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[0;32m~/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/trainer.py:1400\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1400\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1403\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1406\u001b[0m ):\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/trainer.py:1984\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1983\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 1984\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1987\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/trainer.py:2016\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2015\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2016\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:86\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 86\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/code/research/dlkp/venv/lib/python3.8/site-packages/torch/_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\", line 1329, in forward\n    outputs = self.model(\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\", line 1198, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\", line 824, in forward\n    layer_outputs = encoder_layer(\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\", line 307, in forward\n    hidden_states, attn_weights, _ = self.self_attn(\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/debanjan/code/research/dlkp/venv/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\", line 215, in forward\n    attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\nRuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 23.69 GiB total capacity; 6.76 GiB already allocated; 75.56 MiB free; 6.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "KeyphraseGenerator.train_and_eval(model_args, data_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = KeyphraseGenerator.load(\n",
    "    \"../outputs/generation\"\n",
    ")\n",
    "\n",
    "input_text = (\n",
    "    \"In this work, we explore how to learn task-specific language models aimed towards learning rich \"\n",
    "    \"representation of keyphrases from text documents. We experiment with different masking strategies for \"\n",
    "    \"pre-training transformer language models (LMs) in discriminative as well as generative settings. In the \"\n",
    "    \"discriminative setting, we introduce a new pre-training objective - Keyphrase Boundary Infilling with \"\n",
    "    \"Replacement (KBIR), showing large gains in performance (upto 9.26 points in F1) over SOTA, when LM \"\n",
    "    \"pre-trained using KBIR is fine-tuned for the task of keyphrase extraction. In the generative setting, we \"\n",
    "    \"introduce a new pre-training setup for BART - KeyBART, that reproduces the keyphrases related to the \"\n",
    "    \"input text in the CatSeq format, instead of the denoised original input. This also led to gains in \"\n",
    "    \"performance (upto 4.33 points in F1@M) over SOTA for keyphrase generation. Additionally, we also \"\n",
    "    \"fine-tune the pre-trained language models on named entity recognition (NER), question answering (QA), \"\n",
    "    \"relation extraction (RE), abstractive summarization and achieve comparable performance with that of the \"\n",
    "    \"SOTA, showing that learning rich representation of keyphrases is indeed beneficial for many other \"\n",
    "    \"fundamental NLP tasks.\"\n",
    ")\n",
    "\n",
    "generator_out = generator.generate(input_text)\n",
    "print(generator_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
